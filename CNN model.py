# -*- coding: utf-8 -*-
"""Machine Learning [HW4-2].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16pmDgrvikFyzF_HNUyJeaY-1s8MuCrtH

**HW4**
"""

from keras.datasets import cifar10
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten
from keras.utils import np_utils
from sklearn import metrics
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

# A) Importing (Loading) the Dataset:
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

# B) Preprocessing the data:
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train /= 255
X_test /= 255

nb_classes=10
Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

# C) Building The Model:

# ----- Set up the layers ------

model = Sequential()
model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(32, 32, 3)))
model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.2))

model.add(Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))
model.add(Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.2))

model.add(Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))
model.add(Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.2))

model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))


# ------ Compile the model ------
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

# D) Training the model

history = model.fit(X_train, Y_train, validation_split=0.2, batch_size=256, epochs=10)

# E) Making prediction on test set.

y_pred = model.predict(X_test)
y_true = np.argmax(Y_test, axis=1)
y_predicted = np.argmax(y_pred, axis=1)

# F) Evaluating model performance on test set:


# -------- Plotting confusion matrix for each class -------------

CN = metrics.confusion_matrix(y_true, y_predicted)

indices = ['Airplanes', 'Cars', 'Birds', 'Cats', 'Deer', 'Dogs', 'Frogs', 'Horses', 'Ships', 'Trucks'] 
c = ['Airplanes', 'Cars', 'Birds', 'Cats', 'Deer', 'Dogs', 'Frogs', 'Horses', 'Ships', 'Trucks']

CN_dataframe = pd.DataFrame(CN, indices, c)

sns.heatmap(CN_dataframe, annot=True, fmt='d', cmap='Blues')
plt.title(' The Confusion Matrix')
plt.ylabel('Real Values')
plt.xlabel('Predicted Values')
plt.show()

# -------- The precision --------
precision= metrics.precision_score(y_true, y_predicted, average='weighted')

# -------- The Recall --------
recall = metrics.recall_score(y_true, y_predicted, average='weighted')

# -------- The Accuracy  --------
accuracy = metrics.accuracy_score(y_true, y_predicted)

print("The precision: ", precision*100)
print("The Recall: ", recall*100)
print("The accuracy: ", accuracy*100)

#precision, recall and f1-score for each class
print('\nClassification Report\n')
print(metrics.classification_report(y_true, y_predicted, target_names=['Airplanes', 'Cars', 'Birds', 'Cats', 'Deer', 'Dogs', 'Frogs', 'Horses', 'Ships', 'Trucks']))